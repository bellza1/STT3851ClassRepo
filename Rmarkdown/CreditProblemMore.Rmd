---
title: "Credit Problem--More"
author: "Alan Arnholt"
date: 'Last updated: `r format(Sys.time(), "%b %d, %Y")`'
bibliography: ./BIB/CV.bib
output:
  bookdown::html_document2:
    highlight: textmate
    theme: yeti
---

```{r, label = "SETUP", echo = FALSE, results= 'hide', message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(comment = NA, fig.align = 'center', fig.height = 5, fig.width = 5, prompt = FALSE, highlight = TRUE, tidy = FALSE, warning = FALSE, message = FALSE, tidy.opts=list(blank = TRUE, width.cutoff = 80))
```

NOTE: Text in many places is verbatim from chapter 6 of @james_introduction_2013.

# Read in the data: 

```{r}
Credit <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv")
Credit$Utilization <- Credit$Balance / (Credit$Income*100)
summary(Credit)
Credit <- Credit[ ,-1]
DT::datatable(Credit, rownames = FALSE)
```


# Best subsets regression 

To perform *best subset selection*, we fit a separate least squares regression for each possible combination of the *p* predictors.  That is, we fit all *p* models that contain exactly on predictor, all $\binom{p}{2}=p(p - 1)/2$ models that contain exactly two predictors, and so forth.  We then look at all of the resulting models, with the goal of identifying the one that is *best*.

## Algorithm 6.1 *Best subset selection* {-}

1. Let $\mathcal{M}_0$ denote the *null model*, which contains no predictors.  This model simply predicts the sample mean for each observation.

2. For $k = 1, 2, \ldots, p:$

     (a) Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors.
     (b) Pick the best among these $\binom{p}{k}$ models, and call it $\mathcal{M}_k$.  Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.
  
3. Select a single best model from among the $\mathcal{M}_0, \ldots, \mathcal{M}_p$ using cross validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.   


The `regsubsets()` function (part of the `leaps` package) performs best subset selection by identifying the best model that contains a given number of predictors, where *best* is quantified using RSS.  The syntax is the same as for `lm()`.  The `summary()` command outputs the best set of variables for each model size. 

Note that the default for `nvmax` (maximum size of subsets to examine) is 8.  To evaluate all of the variables in `Credit`, use `nvmax = 12`.

```{r}
library(leaps)
regfit.full <- regsubsets(Balance ~. , data = Credit, nvmax = 12)
summary(regfit.full)
```

An asterisk indicates that a given variable is included in the corresponding model.  For instance, this output indicates that the best two-variable model contains only `Rating` and `Utilization`.  

The `summary()` function also returns $R^2$, RSS, adjusted $R^2$, $C_p$, and BIC.  

```{r}
names(summary(regfit.full))
summary(regfit.full)$rsq
reg.summary <- summary(regfit.full)
reg.summary$bic
reg.summary$rss
```



## Plotting

Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all models at once will help us decide which model to select.  The `points()` command works like the `plot()` command, except that it puts points on a plot that has already been created, instead of creating a new plot.  The `which.max()` function can be used to identify the location of the maximum point of a vector.  

```{r, label= "reddot", fig.width=10, fig.height = 12, fig.cap = "Red dots indicate optimal number of variables"}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = " Adjusted RSq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red",cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp",
type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC",
type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
par(mfrow = c(1, 1))
```


## Using the generic `leaps` plotting functions

The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic.

```{r, fig.width=12, fig.height = 12}
par(mfrow = c(2, 2))
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
par(mfrow = c(1, 1))
```

What are the coefficients selected with BIC?

```{r}
which.min(reg.summary$bic)
coef(regfit.full, which.min(reg.summary$bic))
```

## Forward selection with `leaps`

*Forward stepwise selection* is a computationally efficient alternative to best subset selection.  While the best subset selection procedure considers all $2^p$ possible models containing subsets of the $p$ predicts, forward stepwise considers a much smaller set of models.  Forward stepwise selection begins with a model containing no predictors, and then adds predicts to the model, one-at-a-time, until all of the predictors are in the model.  In particular, at each step the variable that gives the greatest *additional* improvement to the fit is added to the model.  

## Algorithm 6.2 *Forward stepwise selection* {-}

1. Let $\mathcal{M}_0$ denote the *null model*, which contains no predictors.  This model simply predicts the sample mean for each observation.

2. For $k = 0, 1, \ldots, p-1:$

     (a) Consider all $p-k$ models that augment the predictors in $\mathcal{M}_k$ with one additional predictor.
     (b) Choose the *best* among these $p-k$ models, and call it $\mathcal{M}_{k+1}$.  Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.
  
3. Select a single best model from among the $\mathcal{M}_0, \ldots, \mathcal{M}_p$ using cross validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.

Unlike best subset selection, which involved fitting $2^p$ models, forward step wise selection involves fitting one null model, along with $p-k$ models in the *k*th iteration, for $k=0,1, \ldots,p-1$.  This amounts to a total of $1 + \sum_{k=0}^{p-1}(p-k)=1 + p(p + 1)/2$ models.  This is a substantial difference: when $p=12$, best subset selection requires fitting $2^{12} = `r 2^12`$ models, whereas forward stepwise selection requires fitting only $1 + 12(12 + 1)= `r 1 + 12*(12+1)`$ models.

```{r}
regfit.fwd <- regsubsets(Balance ~. , data = Credit , nvmax = 12,
method = "forward")
summary(regfit.fwd)
reg.summary <- summary(regfit.fwd)
```

## Plotting

```{r, fig.width=12, fig.height = 12}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = " Adjusted RSq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red",cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp",
type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC",
type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
par(mfrow = c(1, 1))
```

## Using the build in `leaps` plotting functions

```{r, fig.width=10, fig.height = 12}
par(mfrow = c(2, 2))
plot(regfit.fwd, scale = "r2")
plot(regfit.fwd, scale = "adjr2")
plot(regfit.fwd, scale = "Cp")
plot(regfit.fwd, scale = "bic")
par(mfrow = c(1, 1))
```

What are the coefficients selected with BIC?

```{r}
which.min(reg.summary$bic)
coef(regfit.fwd, which.min(reg.summary$bic))
```


## Backward selection with `leaps`

*Backward stepwise selection* like forward stepwise selection provides a computationally efficient alternative to best subset selection.  However, unlike forward stepwise selection, it begins with the full least squares model containing all *p* predictors, and then iteratively removes the least useful predictor, one-at-a-time. 

## Algorithm 6.3 *Backward stepwise selection* {-}

1. Let $\mathcal{M}_p$ denote the *full model*, which contains all *p* predictors.  

2. For $k = p, p-1, \ldots, 1:$

     (a) Consider all *k* models that contain all but one of the predictors in $\mathcal{M}_k$, for a total of $k-1$ predictors.
     (b) Choose the *best* among these $k$ models, and call it $\mathcal{M}_{k-1}$.  Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.
  
3. Select a single best model from among the $\mathcal{M}_0, \ldots, \mathcal{M}_p$ using cross validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.


Like forward stepwise selection, the backward selection approach searches through only $1 + p(p+1)/2$ models, and so can be applied in settings where *p* is too large to apply best subset selection.  Also like forward stepwise selection, backward stepwise selection is not guaranteed to yield the *best* model containing a subset of *p* predictors.

Backward selection requires that the number of samples $n$ is larger than the number of variables $p$ (so that the full model can be fit).  In contrast, forward stepwise can be used even when $n <p$, and so is the only viable method when $p$ is very large.

```{r}
regfit.bwd <- regsubsets(Balance ~. , data = Credit , nvmax = 12, method = "backward")
summary(regfit.bwd)
reg.summary <- summary(regfit.bwd)
```

## Plotting

```{r, fig.width=12, fig.height = 12}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = " Adjusted RSq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red",cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp",
type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC",
type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
par(mfrow = c(1, 1))
```

## Using the build in `leaps` plotting functions

```{r, fig.width=12, fig.height = 12}
par(mfrow = c(2, 2))
plot(regfit.bwd, scale = "r2")
plot(regfit.bwd, scale = "adjr2")
plot(regfit.bwd, scale = "Cp")
plot(regfit.bwd, scale = "bic")
par(mfrow = c(1, 1))
```

What are the coefficients selected with BIC?

```{r}
which.min(reg.summary$bic)
coef(regfit.bwd, which.min(reg.summary$bic))
```

### Different Models?

```{r}
coef(regfit.full, 5)
coef(regfit.fwd, 5)
coef(regfit.bwd, 5)
#
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```


# Choosing among models using validation set approach

We just saw that it is possible to choose among a set of models of different sizes using $C_p$, BIC, and adjusted $R^2$.  We will now consider how to do this using the validation set and cross-validation approaches.  

In order for these approaches to yield accurate estimates of the test error, we must use *only the training observations* to perform all aspects of model fitting---including variable selection.  Therefore, the determination of which model of a given size is best must be made using *only the training observations*.  This point is subtle but important.  If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.

In order to use the validation set approach, we begin by splitting the observations into a training set and a test set.  We do this by creating a random vector, `train`, of elements equal to `TRUE` if the corresponding observation is in the training set, and `FALSE` otherwise.  The vector `test` has a `TRUE` if the observation is in the test set, and a `FALSE` otherwise.  Note the `!` in the command to create `test` causes `TRUE`s to be switched to `FALSE`s and vice versa. 

```{r}
set.seed(134)
train = sample(c(TRUE, FALSE), size = nrow(Credit), replace = TRUE)
test <- (!train)
```

Now, we apply `regsubsets()` to the training set in order to perform **best subset selection**.  Note: this method is not viable for large *p*.

```{r}
regfit.best <- regsubsets(Balance ~ ., data = Credit[train, ], nvmax = 12)
```

Before computing the validation set error for the best model of each model size, a model matrix for the test data is created.

```{r}
test.mat <- model.matrix(Balance ~ ., data = Credit[test, ])
head(test.mat)
```

The `model.matrix()` function is used in many regression packages for building an "X" matrix from data.  Now we run the loop, and for each size `i`, we extract the coefficients from `regfit.best` for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.

```{r}
coef1 <- coef(regfit.best, id = 1)
coef1
coef2 <- coef(regfit.best, id = 2)
coef2
names(coef1)
names(coef2)
head(test.mat[, names(coef1)])
head(test.mat[, names(coef2)])
```


```{r}
val.errors <- numeric(12)
for(i in 1:12){
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)]%*%coefi
  val.errors[i] <- mean((Credit$Balance[test] - pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best, which.min(val.errors))
```

## Creating a `predict` function for `regsubsets`

That last chunk of code was rather tedious, partly because there is no `predict()` method for `regsubsets()`.  Since we will be using this function again, we can capture our steps above and write our own method.

```{r}
predict.regsubsets=function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[ , xvars]%*%coefi
}
```


Finally, we perform best subset selection on the full data set, and select the best `r which.min(val.errors)` variable model.  It is important that we make use of the full data set in order to obtain more accurate coefficient estimates.  Note that we perform best subset selection of the full data set and select the best `r which.min(val.errors)` variable model, rather that simply using the variables that were obtained from the training set, because the best `r which.min(val.errors)`-variable model on the full data set may differ from the corresponding model in the training set.

```{r}
regfit.best <- regsubsets(Balance ~ ., data = Credit, nvmax = which.min(val.errors))
coef(regfit.best, id = which.min(val.errors))
```

## Choosing among models of different sizes with cross validation

Next we choose among the models of different sizes using cross validation.  This approach is somewhat involved, as we must perform best subset selection *within each of the k training sets*.  Despite this, we see that with its clever subsetting syntax, `R` makes this job quite easy.  First we create a vector that allocates each observation to one of $k = 5$ folds, and we create a matrix in which we will store the results.

```{r}
k <- 5
set.seed(1)
folds <- sample(1:k, size = nrow(Credit), replace = TRUE)
cv.errors <- matrix(NA, k, 12, dimnames = list(NULL, paste(1:12)))
```

Now we write a for loop that performs the cross-validation.  In the $j^{\text{th}}$ fold, the elements of `folds` that equal `j` are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new `predict()` method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix `cv.errors`.

```{r}
for(j in 1:k){
  best.fit <- regsubsets(Balance ~ ., data = Credit[folds != j, ], nvmax = 12)
  for(i in 1:12){
    pred <- predict(best.fit, Credit[folds == j, ], id = i)
    cv.errors[j, i] <- mean((Credit$Balance[folds == j] - pred)^2)
  }
}
```

This has given us a $5 \times 12$ matrix, of which the ($i,j$)th element corresponds to the test MSE for the $i^{\text{th}}$ cross-validation fold for the best $j$-variable model.  We use the `apply()` function to average over the columns of this matrix in order to obtain a vector for which the $j^{\text{th}}$ element is the cross validation error for the $j$-variable model.

```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
which.min(mean.cv.errors)
plot(mean.cv.errors, type = "b")
```

Note that the best model contains `r which.min(mean.cv.errors)` variables.  We now perform best subset selection of the **full data** set in order to obtain the `r which.min(mean.cv.errors)`-variable model.

```{r}
reg.best <- regsubsets(Balance ~ ., data = Credit, nvmax = 12)
coef(reg.best, which.min(mean.cv.errors))
coef(reg.best, 3) # The curve really does not drop much after 3...
mymod <- lm(Balance ~ Income + Rating +  Student, data = Credit)
summary(mymod)
```

# Using `stepAIC`

```{r}
library(MASS)
null <- lm(Balance ~ 1, data = Credit)
full <- lm(Balance ~ ., data = Credit)
mod.fs <- stepAIC(null, scope = list(lower = null, upper = full), direction = "forward", test = "F")
mod.be <- stepAIC(full, scope = list(lower = null, upper = full), direction = "backward", test = "F")
summary(mod.fs)
summary(mod.be) 
car::vif(mod.be)
car::vif(mod.fs)
```

# Shrinkage Methods

As an alternative to subset selection, we can fit a model containing all *p* predictors using a technique that *constrains* or *regularizes* the coefficient estimates, or equivalently, that *shrinks* the coefficient estimates towards zero.  It turns out that shrinking the coefficient estimates can significantly reduce their variance.  The two best-known techniques for shrinking the regression coefficients towards zero are *ridge regression* and the *lasso*.  

## Ridge Regression

The least squares fitting procedure estimates $\beta_0, \beta_1, \ldots, \beta_p$ using the values that minimize Equation \@ref(eq:ls).

\begin{equation}
\text{RSS} = \sum_{i=1}^{n}\left(y_i - \beta_0 - \sum_{j=1}^{p}\beta_{j}x_{ij} \right)^2
(\#eq:ls)
\end{equation}

Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity.  In particular, the ridge regression coefficient estimates $\hat{\beta}^{R}$ are the values that minimize Equation \@ref(eq:rr).

\begin{equation}
\sum_{i=1}^{n}\left(y_i - \beta_0 - \sum_{j=1}^{p}\beta_{j}x_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^{p}\beta_j^2
(\#eq:rr)
\end{equation}

where $\lambda \geq 0$ is a *tuning parameter*, to be estimated separately.  As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.  However, the second term $\lambda \sum_{j=1}^{p}\beta_j^2$, called a *shrinkage penalty*, is small when $\beta_1, \ldots, \beta_p$ are close to zero, and so has the effect of *shrinking* the estimates of $\beta_j$ towards zero.  The tuning parameter serves to control the relative impact of these two two terms on the regression coefficient estimates.  When $\lambda = 0$, the penalty term has no effect, and ridge regression will produce the least squares estimates.  However, as $\lambda \rightarrow \infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.  Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates $\hat{\beta}^{R}_{\lambda}$, for each value of $\lambda$.

**Note that the shrinkage penalty in not applied to $\beta_0$.**

### `glmnet()`

The `glmnet()` function has an `alpha` argument that determines what type of model is fit.  If `alpha = 0` then a **ridge regreesion** model is fit, and if `alpha = 1` then a **lasso model** is fit.  By default, the `glmnet()` function standardizes the variables so that they are on the same scale.  To turn off this default setting, use the argument `standardize = FALSE`.  Also by default, the `glmnet()` function performs ridge regression for an automatically selected range of $\lambda$ values.  However, below we have choose to implement the function over a grid of values ranging form $\lambda = 10^{10}$ to $\lambda = 10^{-2}$, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit.  

Associated with each value of $\lambda$ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by `coef()`.  

```{r}
set.seed(1)
train = sample(c(TRUE, FALSE), size = nrow(Credit), replace = TRUE)
test <- (!train)
library(glmnet)
x <- model.matrix(Balance ~ ., data = Credit)[, -1] # Remove (Intercept) column
y <- Credit$Balance
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid)
dim(coef(ridge.mod))
```

In this case, it is a $`r dim(coef(ridge.mod))[1]` \times `r dim(coef(ridge.mod))[2]`$ matrix, with `r dim(coef(ridge.mod))[1]` rows (one for each predictor, plus an intercept) and `r dim(coef(ridge.mod))[2]` columns (one for each value of $\lambda$).  We expect the coefficient estimates to be much smaller, in terms of $\mathcal{l}_2$ norm, when a large value of $\lambda$ is used, as compared to when a small value of $\lambda$ is used.

```{r}
grid[c(25, 75)]
```

These are the coefficients  when $\lambda = `r grid[25]`$, along with their $\mathcal{l}_2$ norm.

```{r}
ridge.mod$lambda[25]  # lambda
coef(ridge.mod)[, 25]
sqrt(sum(coef(ridge.mod)[-1, 25]^2)) # l_2 norm
```

In contrast, here are the coefficients when $\lambda = `r grid[75]`$, along with their $\mathcal{l}_2$ norm.  Note the much larger $\mathcal{l}_2$ norm of the coefficients associated with this smaller value of $\lambda$.

```{r}
ridge.mod$lambda[75]  # lambda
coef(ridge.mod)[, 75]
sqrt(sum(coef(ridge.mod)[-1, 75]^2)) # l_2 norm
```



```{r}
plot(ridge.mod, xvar = "lambda", label = TRUE)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlambda <- cv.out$lambda.min
bestlambda
ridge.pred <- predict(ridge.mod, s = bestlambda, newx = x[test, ])
mean((ridge.pred - y[test])^2)
final <- glmnet(x, y, alpha = 0)
predict(final, type = "coefficients", s = bestlambda)
```

# Lasso Regression

```{r}
x <- model.matrix(Balance ~ ., data = Credit)[, -1]
y <- Credit$Balance
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[train,], y[train], lambda = grid)
dim(coef(lasso.mod))
plot(lasso.mod, xvar = "lambda", label = TRUE)
plot(lasso.mod, xvar = "dev", label = TRUE)
set.seed(123)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlambda <- cv.out$lambda.min
bestlambda
lasso.pred <- predict(lasso.mod, s = bestlambda, newx = x[test, ])
mean((lasso.pred - y[test])^2)
final <- glmnet(x, y, alpha = 1, lambda = grid)
predict(final, type = "coefficients", s = bestlambda)
predict(final, type = "coefficients", s = 20)
```


# Changing the problem now
## Response is now `Rating`

* Create a model that predicts `Rating` with `Limit`, `Cards`, `Married`, `Student`, and `Education` as features. 

```{r, fig.width = 12, fig.height = 12}
mod <- lm(Rating ~ Limit + Cards + Married + Student + Education, data = Credit)
summary(mod)
par(mfrow = c(2, 2))
plot(mod)
par(mfrow = c(1, 1))
car::residualPlots(mod)
modN <- lm(Rating ~ poly(Limit, 2, raw = TRUE) + poly(Cards, 2, raw = TRUE) + Married + Student + Education, data = Credit)
summary(modN)
car::residualPlots(modN)
car::vif(modN)
summary(modN)
```

* Use your model to predict the `Rating` for an individual that has a credit card limit of
$6,000, has 4 credit cards, is married, and is not a student, and has an undergraduate degree (`Education` = 16).

* Use your model to predict the `Rating` for an individual that has a credit card limit of
$12,000, has 2 credit cards, is married, is not a student, and has an eighth grade education (`Education` = 8).

```{r}
predict(modN, newdata = data.frame(Limit = 6000, Cards = 4, Married = "Yes", Student = "No", Education = 16), response = "pred")
### Should be the same as:
coef(modN)[1] + coef(modN)[2]*6000 + coef(modN)[3]*6000^2 + coef(modN)[4]*4 + coef(modN)[5]*4^2 + coef(modN)[6]*1 + coef(modN)[7]*0 + coef(modN)[8]*16
predict(modN, newdata = data.frame(Limit = 12000, Cards = 2, Married = "Yes", Student = "No", Education = 8), response = "pred")
```

## References